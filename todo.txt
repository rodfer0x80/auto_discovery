filter 200 300 and 400 from requests and split tooling ouput into ordered
folder not only the 200
need to properly modulate scripts for each function, then have an oracle that
puts everything together and builds report, reads report and writes visual.log
modulate external_tools, tools, oracle and threadpool(make it more flexible)
lib should be turned into moduled tools like rice again, we will expand them
and make them big enough for a folder
modulate scripts for each funcionality and incorporate tools like ffuf, sqlmap,
rice_again, subfinder, nmap, robots and sitemap getters, framework enumerator
incorporatted  into rice_again and make this tools part of the project as well
as scripts to autopwn targets, this recon phase can take hours, its fine, we
will be using a large cloud cluster for this, make is very accurate and when
save it as {target}-{date}-report
make some nice wordlists fore each tool
make a folder of wordlists
make a folder of report with a folder with files with data inside for each tool 
then run a final scanner to make a visual map of the area to explore and the
files where this data is on a final file called like visual.log
change requests headers to --no-leak
